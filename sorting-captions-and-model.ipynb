{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8193427,"sourceType":"datasetVersion","datasetId":4852651},{"sourceId":8193662,"sourceType":"datasetVersion","datasetId":4852846}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T10:58:12.434686Z","iopub.execute_input":"2024-04-22T10:58:12.435768Z","iopub.status.idle":"2024-04-22T10:58:13.764349Z","shell.execute_reply.started":"2024-04-22T10:58:12.435733Z","shell.execute_reply":"2024-04-22T10:58:13.763152Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dataaaa/train_trial1.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:06:55.098488Z","iopub.execute_input":"2024-04-22T13:06:55.099196Z","iopub.status.idle":"2024-04-22T13:06:56.159663Z","shell.execute_reply.started":"2024-04-22T13:06:55.099157Z","shell.execute_reply":"2024-04-22T13:06:56.158476Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataaaa/train_trial1.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:07:22.008980Z","iopub.execute_input":"2024-04-22T13:07:22.009751Z","iopub.status.idle":"2024-04-22T13:07:22.037083Z","shell.execute_reply.started":"2024-04-22T13:07:22.009716Z","shell.execute_reply":"2024-04-22T13:07:22.036123Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.dropna(subset=['captions'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:07:23.542475Z","iopub.execute_input":"2024-04-22T13:07:23.542876Z","iopub.status.idle":"2024-04-22T13:07:23.561926Z","shell.execute_reply.started":"2024-04-22T13:07:23.542848Z","shell.execute_reply":"2024-04-22T13:07:23.560629Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in data['likes']:\n    if \",\" not in i:\n        l.append(int(i))\n    else:\n        a = i.split(\",\")\n        l.append(int(\"\".join(a)))\ndata['likes'] = l","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:07:24.363662Z","iopub.execute_input":"2024-04-22T13:07:24.364070Z","iopub.status.idle":"2024-04-22T13:07:24.371703Z","shell.execute_reply.started":"2024-04-22T13:07:24.364042Z","shell.execute_reply":"2024-04-22T13:07:24.370297Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define neural network architecture with classification head\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import BertTokenizer, BertModel\n\n# Load pre-trained BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nclass CaptionLikesPredictor(nn.Module):\n    def __init__(self, bert_model, num_classes):\n        super(CaptionLikesPredictor, self).__init__()\n        self.bert = bert_model\n        self.fc = nn.Linear(768, num_classes)  # Output dimension is number of classes\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        class_probs = self.fc(pooled_output)\n        return class_probs\n\n# Prepare data\n# Assuming you have a DataFrame called 'data' with columns 'caption' and 'likes'\n# Identify the most liked caption for each sample\n# Identify the most liked caption for each sample\nmost_liked_caption_indices = data.groupby('Unnamed: 0')['likes'].idxmax()\ndata['is_most_liked'] = 0\ndata.loc[most_liked_caption_indices, 'is_most_liked'] = 1\n\n\n# Split data into train and test sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Tokenize captions\ndef tokenize_text(text):\n    return tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n\ntrain_data['tokenized'] = train_data['captions'].apply(tokenize_text)\ntest_data['tokenized'] = test_data['captions'].apply(tokenize_text)\n\n# Convert DataFrame to PyTorch Dataset\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n        input_ids = item['tokenized']['input_ids'].squeeze(0)\n        attention_mask = item['tokenized']['attention_mask'].squeeze(0)\n        is_most_liked = torch.tensor(item['is_most_liked'], dtype=torch.long)\n        return input_ids, attention_mask, is_most_liked\n\ntrain_dataset = CustomDataset(train_data)\ntest_dataset = CustomDataset(test_data)\n\n# Define DataLoader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Initialize model\nmodel = CaptionLikesPredictor(bert_model, num_classes=2)  # 2 classes: most liked or not\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    for input_ids, attention_mask, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        test_losses = []\n        for input_ids, attention_mask, labels in test_loader:\n            outputs = model(input_ids, attention_mask)\n            test_loss = criterion(outputs, labels)\n            test_losses.append(test_loss.item())\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Test Loss: {np.mean(test_losses)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:09:03.079162Z","iopub.execute_input":"2024-04-22T13:09:03.079619Z","iopub.status.idle":"2024-04-22T13:09:03.239634Z","shell.execute_reply.started":"2024-04-22T13:09:03.079584Z","shell.execute_reply":"2024-04-22T13:09:03.238073Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load pre-trained BERT tokenizer and model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCaptionLikesPredictor\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2070\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2065\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2073\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2074\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2075\u001b[0m     )\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2078\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."],"ename":"OSError","evalue":"Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.","output_type":"error"}]},{"cell_type":"code","source":"torch.save(model,'/kaggle/working/modelcap.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:26:04.197491Z","iopub.execute_input":"2024-04-22T11:26:04.198267Z","iopub.status.idle":"2024-04-22T11:26:04.833852Z","shell.execute_reply.started":"2024-04-22T11:26:04.198222Z","shell.execute_reply":"2024-04-22T11:26:04.832750Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def prepare_caption(text):\n    return tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, truncation=True, return_tensors='pt')\n\n# Define a function to predict probabilities for each caption\ndef predict_probability(model, tokenized_caption):\n    with torch.no_grad():\n        input_ids = tokenized_caption['input_ids'].squeeze(0)\n        attention_mask = tokenized_caption['attention_mask'].squeeze(0)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        probabilities = torch.softmax(outputs.logits, dim=1)[0][1].item()  # Assuming 1 is the index for 'most liked'\n    \n    return probabilities","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:25:26.107816Z","iopub.execute_input":"2024-04-22T11:25:26.111977Z","iopub.status.idle":"2024-04-22T11:25:26.131777Z","shell.execute_reply.started":"2024-04-22T11:25:26.111869Z","shell.execute_reply":"2024-04-22T11:25:26.130628Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/captiondataset/final_caps.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:49:13.942157Z","iopub.execute_input":"2024-04-22T11:49:13.942650Z","iopub.status.idle":"2024-04-22T11:49:13.954875Z","shell.execute_reply.started":"2024-04-22T11:49:13.942599Z","shell.execute_reply":"2024-04-22T11:49:13.953232Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:49:15.377552Z","iopub.execute_input":"2024-04-22T11:49:15.378091Z","iopub.status.idle":"2024-04-22T11:49:15.398841Z","shell.execute_reply.started":"2024-04-22T11:49:15.378045Z","shell.execute_reply":"2024-04-22T11:49:15.397066Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                                         family trip  \\\n0              My family will always travel together   \n1  Seeing your kids smile makes all the stress an...   \n2  I would travel my entire life if I could alway...   \n3  I’ll keep these memories in my heart until our...   \n4  No matter where we go in the world, we will al...   \n\n                                friends trip  \\\n0                  I’ve found my happy place   \n1       What’s on my bucket list? Everywhere   \n2  Take only pictures, leave only footprints   \n3           Always say yes to new adventures   \n4        Life is short and the world is wide   \n\n                                        abstract art  \\\n0                                       Spot the CEO   \n1  'The greatest glory in living lies not in neve...   \n2  My favorite exercise is a cross between a lung...   \n3  Patience — what you have when there are too ma...   \n4                       Don’t give up on your dreams   \n\n                                             meeting               wedding  \\\n0  “Right now, I am confused about every single t...      Chasing daylight   \n1                            Better than Pretzel Day  Hues of tranquillity   \n2                     \"I never doubted you were mine        Fiery farewell   \n3  “If you care about someone, and you got a litt...    Cotton candy skies   \n4            “Marry me… because I'd like to date you    Sunkissed evenings   \n\n                                              sunset  \\\n0                     A new morning, a new beginning   \n1  All of the letdowns of yesterday are redeemed ...   \n2  And in the middle of waiting for sunrise, I fo...   \n3            Another sunrise granted by the almighty   \n4          Best kind of gold! Plus, it can’t be sold   \n\n                          fitness                                   scenery  \n0                    © 2024 Later            Living in the rhythm of nature  \n1                    © 2024 Later       Impressions of untouched wilderness  \n2  This Barbie got her workout in      The dance of the sea meeting the sky  \n3                     Just peachy  Mountain melodies whispering in the wind  \n4           Omg I'm hot girl fit            Views that leave you speechless  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family trip</th>\n      <th>friends trip</th>\n      <th>abstract art</th>\n      <th>meeting</th>\n      <th>wedding</th>\n      <th>sunset</th>\n      <th>fitness</th>\n      <th>scenery</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My family will always travel together</td>\n      <td>I’ve found my happy place</td>\n      <td>Spot the CEO</td>\n      <td>“Right now, I am confused about every single t...</td>\n      <td>Chasing daylight</td>\n      <td>A new morning, a new beginning</td>\n      <td>© 2024 Later</td>\n      <td>Living in the rhythm of nature</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Seeing your kids smile makes all the stress an...</td>\n      <td>What’s on my bucket list? Everywhere</td>\n      <td>'The greatest glory in living lies not in neve...</td>\n      <td>Better than Pretzel Day</td>\n      <td>Hues of tranquillity</td>\n      <td>All of the letdowns of yesterday are redeemed ...</td>\n      <td>© 2024 Later</td>\n      <td>Impressions of untouched wilderness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I would travel my entire life if I could alway...</td>\n      <td>Take only pictures, leave only footprints</td>\n      <td>My favorite exercise is a cross between a lung...</td>\n      <td>\"I never doubted you were mine</td>\n      <td>Fiery farewell</td>\n      <td>And in the middle of waiting for sunrise, I fo...</td>\n      <td>This Barbie got her workout in</td>\n      <td>The dance of the sea meeting the sky</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I’ll keep these memories in my heart until our...</td>\n      <td>Always say yes to new adventures</td>\n      <td>Patience — what you have when there are too ma...</td>\n      <td>“If you care about someone, and you got a litt...</td>\n      <td>Cotton candy skies</td>\n      <td>Another sunrise granted by the almighty</td>\n      <td>Just peachy</td>\n      <td>Mountain melodies whispering in the wind</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>No matter where we go in the world, we will al...</td>\n      <td>Life is short and the world is wide</td>\n      <td>Don’t give up on your dreams</td>\n      <td>“Marry me… because I'd like to date you</td>\n      <td>Sunkissed evenings</td>\n      <td>Best kind of gold! Plus, it can’t be sold</td>\n      <td>Omg I'm hot girl fit</td>\n      <td>Views that leave you speechless</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\n\n# Load pre-trained BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Define a function to tokenize and prepare captions\ndef prepare_caption(text):\n    return tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, truncation=True, return_tensors='pt')\n\n# Example set of captions\ncaptions = list(df['family trip'])\n\n# Tokenize and prepare captions\ntokenized_captions = [prepare_caption(caption) for caption in captions]\n\n# Predict probabilities for each caption\n# Predict probabilities for each caption\ndef predict_probabilities(model, tokenized_captions):\n    probabilities = []\n    with torch.no_grad():\n        for tokenized_caption in tokenized_captions:\n            input_ids = tokenized_caption['input_ids'].squeeze(0)\n            attention_mask = tokenized_caption['attention_mask'].squeeze(0)\n\n            # Pad or truncate input_ids to a fixed length\n            max_length = 128\n            input_ids = torch.cat([input_ids, torch.zeros(max_length - input_ids.shape[0], dtype=torch.long)])\n            attention_mask = torch.cat([attention_mask, torch.zeros(max_length - attention_mask.shape[0], dtype=torch.long)])\n\n            # Ensure input_ids and attention_mask have correct shape\n            input_ids = input_ids.unsqueeze(0)\n            attention_mask = attention_mask.unsqueeze(0)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            probabilities.append(torch.softmax(outputs, dim=1)[0][1].item())  # Assuming 1 is the index for 'most liked'\n    return probabilities\n\n\n# Load your trained model\n# loaded_model = YourLoadedModelClass(bert_model, num_classes=2)  # Example\n\n# Predict probabilities\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nprint(results)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:49:26.227145Z","iopub.execute_input":"2024-04-22T11:49:26.227586Z","iopub.status.idle":"2024-04-22T11:50:29.274045Z","shell.execute_reply.started":"2024-04-22T11:49:26.227551Z","shell.execute_reply":"2024-04-22T11:50:29.272736Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"                                               Caption  Probability\n0                My family will always travel together     0.993031\n1    Seeing your kids smile makes all the stress an...     0.992589\n2    I would travel my entire life if I could alway...     0.992488\n3    I’ll keep these memories in my heart until our...     0.991265\n4    No matter where we go in the world, we will al...     0.992127\n..                                                 ...          ...\n195                  Life is far too short for routine     0.993283\n196                Wasted time here is time well spent     0.993320\n197     I haven't been everywhere, but it's on my list     0.993493\n198                          I collect passport stamps     0.992503\n199  Whenever I regret the money I spent traveling,...     0.991715\n\n[200 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"results = results.sort_values(by='Probability',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:50:41.915177Z","iopub.execute_input":"2024-04-22T11:50:41.916033Z","iopub.status.idle":"2024-04-22T11:50:41.923485Z","shell.execute_reply.started":"2024-04-22T11:50:41.915987Z","shell.execute_reply":"2024-04-22T11:50:41.922084Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:50:42.950871Z","iopub.execute_input":"2024-04-22T11:50:42.951252Z","iopub.status.idle":"2024-04-22T11:50:42.965018Z","shell.execute_reply.started":"2024-04-22T11:50:42.951223Z","shell.execute_reply":"2024-04-22T11:50:42.963671Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"                                               Caption  Probability\n165                 Exploring hearts, exploring places     0.995198\n164                            Travel and togetherness     0.994702\n120   Fun and laughter, all day, every day! <img al...     0.994618\n127               Exploring new horizons, hand in hand     0.994581\n147                 Family and travel, a perfect blend     0.994547\n..                                                 ...          ...\n83      “Don’t just tell your children about the world     0.988519\n177     “Don’t just tell your children about the world     0.988519\n97                  “Having kids is a reason to travel     0.987365\n188                              “Don’t count the days     0.987057\n71                  “You can’t say we didn’t live life     0.982898\n\n[200 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Caption</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>165</th>\n      <td>Exploring hearts, exploring places</td>\n      <td>0.995198</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>Travel and togetherness</td>\n      <td>0.994702</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>Fun and laughter, all day, every day! &lt;img al...</td>\n      <td>0.994618</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>Exploring new horizons, hand in hand</td>\n      <td>0.994581</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>Family and travel, a perfect blend</td>\n      <td>0.994547</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>“Don’t just tell your children about the world</td>\n      <td>0.988519</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>“Don’t just tell your children about the world</td>\n      <td>0.988519</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>“Having kids is a reason to travel</td>\n      <td>0.987365</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>“Don’t count the days</td>\n      <td>0.987057</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>“You can’t say we didn’t live life</td>\n      <td>0.982898</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_df = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:51:31.776308Z","iopub.execute_input":"2024-04-22T11:51:31.776858Z","iopub.status.idle":"2024-04-22T11:51:31.785432Z","shell.execute_reply.started":"2024-04-22T11:51:31.776824Z","shell.execute_reply":"2024-04-22T11:51:31.783555Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"new_df['family trip'] = results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:51:35.597838Z","iopub.execute_input":"2024-04-22T11:51:35.598298Z","iopub.status.idle":"2024-04-22T11:51:35.606464Z","shell.execute_reply.started":"2024-04-22T11:51:35.598267Z","shell.execute_reply":"2024-04-22T11:51:35.604840Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:51:39.450955Z","iopub.execute_input":"2024-04-22T11:51:39.451411Z","iopub.status.idle":"2024-04-22T11:51:39.467566Z","shell.execute_reply.started":"2024-04-22T11:51:39.451380Z","shell.execute_reply":"2024-04-22T11:51:39.465890Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                                           family trip\n165                 Exploring hearts, exploring places\n164                            Travel and togetherness\n120   Fun and laughter, all day, every day! <img al...\n127               Exploring new horizons, hand in hand\n147                 Family and travel, a perfect blend\n..                                                 ...\n83      “Don’t just tell your children about the world\n177     “Don’t just tell your children about the world\n97                  “Having kids is a reason to travel\n188                              “Don’t count the days\n71                  “You can’t say we didn’t live life\n\n[200 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family trip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>165</th>\n      <td>Exploring hearts, exploring places</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>Travel and togetherness</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>Fun and laughter, all day, every day! &lt;img al...</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>Exploring new horizons, hand in hand</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>Family and travel, a perfect blend</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>“Don’t just tell your children about the world</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>“Don’t just tell your children about the world</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>“Having kids is a reason to travel</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>“Don’t count the days</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>“You can’t say we didn’t live life</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:47:23.548617Z","iopub.execute_input":"2024-04-22T11:47:23.549108Z","iopub.status.idle":"2024-04-22T11:47:23.558766Z","shell.execute_reply.started":"2024-04-22T11:47:23.549074Z","shell.execute_reply":"2024-04-22T11:47:23.557149Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Index(['family trip', 'friends trip', 'abstract art', 'meeting', 'wedding',\n       'sunset', 'fitness', 'scenery'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"captions = list(df['friends trip'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['friends trip']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:51:51.943774Z","iopub.execute_input":"2024-04-22T11:51:51.944230Z","iopub.status.idle":"2024-04-22T11:52:52.239976Z","shell.execute_reply.started":"2024-04-22T11:51:51.944200Z","shell.execute_reply":"2024-04-22T11:52:52.238451Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:53:01.261765Z","iopub.execute_input":"2024-04-22T11:53:01.262631Z","iopub.status.idle":"2024-04-22T11:53:01.278733Z","shell.execute_reply.started":"2024-04-22T11:53:01.262575Z","shell.execute_reply":"2024-04-22T11:53:01.277045Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                                           family trip  \\\n165                 Exploring hearts, exploring places   \n164                            Travel and togetherness   \n120   Fun and laughter, all day, every day! <img al...   \n127               Exploring new horizons, hand in hand   \n147                 Family and travel, a perfect blend   \n..                                                 ...   \n83      “Don’t just tell your children about the world   \n177     “Don’t just tell your children about the world   \n97                  “Having kids is a reason to travel   \n188                              “Don’t count the days   \n71                  “You can’t say we didn’t live life   \n\n                                          friends trip  \n165  “Because of you, I laugh a little louder and s...  \n164  “Friendship consists in forgetting what one gi...  \n120  “A day spent with friends is always a day well...  \n127          “With friends, every day is a celebration  \n147           “A true friend is one soul in two bodies  \n..                                                 ...  \n83         “We’re the ‘let’s just wing it’ specialists  \n177      “When worst comes to worst, squad comes first  \n97                 “Forever grateful for these moments  \n188               “Sisters at heart, partners in crime  \n71         “She’s my ‘let’s do something stupid’ buddy  \n\n[200 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family trip</th>\n      <th>friends trip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>165</th>\n      <td>Exploring hearts, exploring places</td>\n      <td>“Because of you, I laugh a little louder and s...</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>Travel and togetherness</td>\n      <td>“Friendship consists in forgetting what one gi...</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>Fun and laughter, all day, every day! &lt;img al...</td>\n      <td>“A day spent with friends is always a day well...</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>Exploring new horizons, hand in hand</td>\n      <td>“With friends, every day is a celebration</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>Family and travel, a perfect blend</td>\n      <td>“A true friend is one soul in two bodies</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>“Don’t just tell your children about the world</td>\n      <td>“We’re the ‘let’s just wing it’ specialists</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>“Don’t just tell your children about the world</td>\n      <td>“When worst comes to worst, squad comes first</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>“Having kids is a reason to travel</td>\n      <td>“Forever grateful for these moments</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>“Don’t count the days</td>\n      <td>“Sisters at heart, partners in crime</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>“You can’t say we didn’t live life</td>\n      <td>“She’s my ‘let’s do something stupid’ buddy</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"captions = list(df['abstract art'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['abstract art']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:53:03.903436Z","iopub.execute_input":"2024-04-22T11:53:03.904227Z","iopub.status.idle":"2024-04-22T11:54:03.808395Z","shell.execute_reply.started":"2024-04-22T11:53:03.904181Z","shell.execute_reply":"2024-04-22T11:54:03.806985Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"captions = list(df['meeting'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['meeting']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:54:08.293858Z","iopub.execute_input":"2024-04-22T11:54:08.294302Z","iopub.status.idle":"2024-04-22T11:55:08.520173Z","shell.execute_reply.started":"2024-04-22T11:54:08.294267Z","shell.execute_reply":"2024-04-22T11:55:08.518879Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"captions = list(df['wedding'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['wedding']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:55:14.379375Z","iopub.execute_input":"2024-04-22T11:55:14.380231Z","iopub.status.idle":"2024-04-22T11:56:14.600736Z","shell.execute_reply.started":"2024-04-22T11:55:14.380191Z","shell.execute_reply":"2024-04-22T11:56:14.599074Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"captions = list(df['sunset'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['sunset']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:56:17.353366Z","iopub.execute_input":"2024-04-22T11:56:17.353843Z","iopub.status.idle":"2024-04-22T11:57:17.631156Z","shell.execute_reply.started":"2024-04-22T11:56:17.353810Z","shell.execute_reply":"2024-04-22T11:57:17.630078Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"captions = list(df['fitness'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['fitness']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:57:21.595526Z","iopub.execute_input":"2024-04-22T11:57:21.596211Z","iopub.status.idle":"2024-04-22T11:58:22.631401Z","shell.execute_reply.started":"2024-04-22T11:57:21.596176Z","shell.execute_reply":"2024-04-22T11:58:22.630202Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"captions = list(df['scenery'])\ntokenized_captions = [prepare_caption(caption) for caption in captions]\npredicted_probabilities = predict_probabilities(model, tokenized_captions)\n\n# Display results\nresults = pd.DataFrame({'Caption': captions, 'Probability': predicted_probabilities})\nresults = results.sort_values(by='Probability',ascending=False)\nnew_df['scenery']=results['Caption']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:58:41.024110Z","iopub.execute_input":"2024-04-22T11:58:41.024805Z","iopub.status.idle":"2024-04-22T11:59:41.819040Z","shell.execute_reply.started":"2024-04-22T11:58:41.024770Z","shell.execute_reply":"2024-04-22T11:59:41.817386Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"new_df.to_csv('/kaggle/working/sorted_captions.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T12:01:08.797532Z","iopub.execute_input":"2024-04-22T12:01:08.799087Z","iopub.status.idle":"2024-04-22T12:01:08.814339Z","shell.execute_reply.started":"2024-04-22T12:01:08.799040Z","shell.execute_reply":"2024-04-22T12:01:08.812775Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df.columns:\n    df[f'{col}_tokenized'] = df[col].apply(prepare_caption)\n\n# Predict probabilities for each caption in each column\nfor col in df.columns:\n    if '_tokenized' in col:\n        df[f'{col}_probability'] = df[col].apply(lambda x: predict_probability(model, x))\n\n# Sort each column based on the predicted probabilities in descending order\nfor col in df.columns:\n    if '_probability' in col:\n        df = df.sort_values(by=col, ascending=False)\n\n# Drop intermediate tokenized columns\ndf = df.drop(columns=[col for col in df.columns if '_tokenized' in col])\n\n# Print or use sorted data as needed\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:38:20.682894Z","iopub.execute_input":"2024-04-22T11:38:20.683407Z","iopub.status.idle":"2024-04-22T11:38:22.038061Z","shell.execute_reply.started":"2024-04-22T11:38:20.683372Z","shell.execute_reply":"2024-04-22T11:38:22.035970Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m----> 2\u001b[0m     df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tokenized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_caption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Predict probabilities for each caption in each column\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n","File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mprepare_caption\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_caption\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3051\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3041\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3042\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3043\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3044\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3048\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3049\u001b[0m )\n\u001b[0;32m-> 3051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:705\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `is_split_into_words=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Input {'input_ids': tensor([[ 101, 2026, 2155, 2097, 2467, 3604, 2362,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."],"ename":"ValueError","evalue":"Input {'input_ids': tensor([[ 101, 2026, 2155, 2097, 2467, 3604, 2362,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}